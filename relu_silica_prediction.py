# -*- coding: utf-8 -*-
"""Copy of NEURAL NETWORK - USE THIS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uv9lAjGKOoMe1nmxuqok9O2NpUMjE2x1
"""

# Commented out IPython magic to ensure Python compatibility.
from pydrive.auth import GoogleAuth 
from pydrive.drive import GoogleDrive 
from google.colab import auth 
from oauth2client.client import GoogleCredentials 
import datetime as dt
  
# Authenticate and create the PyDrive client. 
auth.authenticate_user() 
gauth = GoogleAuth() 
gauth.credentials = GoogleCredentials.get_application_default() 
drive = GoogleDrive(gauth)

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from pandas import Series, DataFrame

import tensorflow as tf
print("Tensorflow version " + tf.__version__)


import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline


from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn import metrics
from sklearn.model_selection import cross_validate
from sklearn.metrics import recall_score
from sklearn.model_selection import KFold

from tensorflow import keras
from tensorflow.keras import layers

print(tf.__version__)
!pip install -q git+https://github.com/tensorflow/docs
import tensorflow_docs as tfdocs
import tensorflow_docs.plots
import tensorflow_docs.modeling
# to get the id part of the file 
id = '1FmocjGvyUReH1K0Yl-p8sBAZLarCA9GM'
  
downloaded = drive.CreateFile({'id':id})  
downloaded.GetContentFile('MiningProcess_Flotation_Plant_Database.csv')   

# %tensorflow_version 2.x
import tensorflow as tf
import timeit

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  print(
      '\n\nThis error most likely means that this notebook is not '
      'configured to use a GPU.  Change this in Notebook Settings via the '
      'command palette (cmd/ctrl-shift-P) or the Edit menu.\n\n')
  raise SystemError('GPU device not found')

def cpu():
  with tf.device('/cpu:0'):
    random_image_cpu = tf.random.normal((100, 100, 100, 3))
    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)
    return tf.math.reduce_sum(net_cpu)

def gpu():
  with tf.device('/device:GPU:0'):
    random_image_gpu = tf.random.normal((100, 100, 100, 3))
    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)
    return tf.math.reduce_sum(net_gpu)
  
# We run each op once to warm up; see: https://stackoverflow.com/a/45067900
cpu()
gpu()

# Run the op several times.
print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '
      '(batch x height x width x channel). Sum of ten runs.')
print('CPU (s):')
cpu_time = timeit.timeit('cpu()', number=10, setup="from __main__ import cpu")
print(cpu_time)
print('GPU (s):')
gpu_time = timeit.timeit('gpu()', number=10, setup="from __main__ import gpu")
print(gpu_time)
print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))

df = pd.read_csv('MiningProcess_Flotation_Plant_Database.csv', decimal=',', sep=',') 
targets_1hr_range = (df['date'] >= '2017-03-10 04:00:00') & (df['date'] <= '2017-09-09 23:00:00')
targets_1hr = df.loc[targets_1hr_range]
targets_1hr = targets_1hr[["% Iron Concentrate", "% Silica Concentrate"]]
targets_1hr = targets_1hr.reset_index()

features_1hr_range  = (df['date'] >= '2017-03-10 03:00:00') & (df['date'] <= '2017-09-09 22:00:00')
features_1hr = df.loc[features_1hr_range]
features_1hr = features_1hr.iloc[:,1:22]
features_1hr = features_1hr.reset_index()

df_1hr = pd.concat([features_1hr,targets_1hr], axis=1, sort=False)


X = df.iloc[:,1:21]
y = df[["% Silica Concentrate"]]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

train_stats = X_train.describe()
train_stats = train_stats.transpose()
train_stats

def norm(x):
  return (x - train_stats['mean']) / train_stats['std']
normed_train_data = norm(X_train)
normed_test_data = norm(X_test)

df

from google.colab import drive
drive.mount('/content/drive')

train_stats['mean'].to_csv("/content/drive/My Drive/Colab Notebooks/MLFall/Practical project/training_mean.csv")
train_stats['std'].to_csv("/content/drive/My Drive/Colab Notebooks/MLFall/Practical project/training_std.csv")

normed_test_data

def build_model():
  model = keras.Sequential([
    layers.Dense(512, activation='relu', input_shape=[len(X_train.keys())]),
    layers.Dense(256, activation='relu'),
    layers.Dense(128, activation='relu'),
    layers.Dense(64, activation='relu'),
    layers.Dense(32, activation='relu'),
    layers.Dense(16, activation='relu'),
    layers.Dense(8, activation='relu'),
    layers.Dense(4, activation='relu'),
    layers.Dense(1)
  ])

  optimizer = tf.keras.optimizers.Adam(0.001)

  model.compile(loss='mse',
                optimizer=optimizer,
                metrics=['mae', 'mse'])
  return model

model = build_model()

EPOCHS = 150

# The patience parameter is the amount of epochs to check for improvement
early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)

early_history = model.fit(normed_train_data, y_train, 
                    epochs=EPOCHS, validation_split = 0.2, verbose=0, 
                    callbacks=[early_stop, tfdocs.modeling.EpochDots()])

plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)
plotter.plot({'Early Stopping': early_history}, metric = "mae")
plt.ylabel('MAE [%Silica Concentrate]')

y_true = y_test
y_pred = model.predict(normed_test_data)

y_pred = pd.DataFrame(data=y_pred, columns=["y_pred"])
y_true.reset_index(drop=True, inplace=True)
y_true.head()
y  = y_true.join(y_pred)
y.head()
y['error'] = y['% Silica Concentrate']-y['y_pred']
y.head()

loss, mae, mse = model.evaluate(normed_test_data, y_test, verbose=2)

print("Testing set Mean Abs Error: {:5.2f} %".format(mae))

sns.set()

plt.figure(figsize=(16, 6))
cmap = sns.cubehelix_palette(rot=-.2, as_cmap=True)
ax = sns.scatterplot(x="% Silica Concentrate", y="y_pred", alpha=0.07,
                    palette=cmap, sizes=(10, 200),
                    data=y)

plt.figure(figsize=(10, 6))
error = y['error']
ax = sns.distplot(error)

from sklearn.metrics import r2_score
r2_score(y_true, y_pred)

drive.mount('/content/drive/', force_remount = True)
model.save('NN.h5')

pip install -q pyyaml h5py

tf.keras.models.save_model('my_model.h5', filepath = /content/drive/)

model.save('/content/drive/My Drive/Models/NN.h5')

